from langchain.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# embeddings
embeddings = OpenAIEmbeddings()

# vector store
vector_store = Chroma(
    persist_directory="my_vector_store",
    embedding_function=embeddings,
)

# retriever
retriever = vector_store.as_retriever()

# LLM
llm = ChatOpenAI(model="gpt-4o-mini")

# prompt
prompt = ChatPromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ ì§ˆë¬¸ ë‹µë³€ ì‘ì—…ì˜ ë³´ì¡°ìì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
ë‹µë³€ì€ ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

Question: {question}
Context: {context}
Answer: """
)

# question
question = "í•˜ì–€ìƒ‰ ì œí’ˆì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?"
print(f"ğŸ¦Š ì§ˆë¬¸: {question}")

# retrieved docs
retrieved_docs = retriever.invoke(question)

# context
context = "\n".join([doc.page_content for doc in retrieved_docs])

# chain
chain = prompt | llm
res = chain.invoke({"context": context, "question": question})
print(f"ğŸ¤– ë‹µë³€: {res.content}")
